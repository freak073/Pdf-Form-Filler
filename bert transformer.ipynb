{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import fitz  # PyMuPDF for PDF processing\n",
    "import re  # For knowledge base parsing\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pytesseract\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from transformers import BertTokenizer, BertModel, BertForQuestionAnswering\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Set the path to the Tesseract executable\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'  # Adjust this path as needed\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')  # Corrected model name\n",
    "model = BertModel.from_pretrained('bert-base-uncased')  # Corrected model name\n",
    "\n",
    "# Step 1: Parse the Knowledge Base (dummy_data.txt)\n",
    "def parse_knowledge_base(file_path):\n",
    "    knowledge_base = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            match = re.match(r'(.+?):\\s*(.+)', line)\n",
    "            if match:\n",
    "                key, value = match.groups()\n",
    "                knowledge_base[key.strip()] = value.strip()\n",
    "    return knowledge_base\n",
    "\n",
    "# Step 2: Extract Form Fields from PDF\n",
    "def extract_form_fields_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    form_fields = []\n",
    "    for page_num in range(doc.page_count):\n",
    "        page = doc.load_page(page_num)\n",
    "        blocks = page.get_text(\"blocks\")\n",
    "        for block in blocks:\n",
    "            if len(block) >= 5:\n",
    "                x0, y0, x1, y1 = block[:4]\n",
    "                field_text = block[4].strip()\n",
    "                if field_text:\n",
    "                    form_fields.append({\n",
    "                        \"page\": page_num + 1,\n",
    "                        \"coordinates\": (x0, y0, x1, y1),\n",
    "                        \"field_name\": field_text\n",
    "                    })\n",
    "    return form_fields\n",
    "\n",
    "# Step 3: Use BERT to encode text\n",
    "def encode_text(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Mean pooling on token embeddings\n",
    "    embeddings = torch.mean(outputs.last_hidden_state, dim=1)\n",
    "    return embeddings\n",
    "\n",
    "# Step 4: Map Form Fields to Knowledge Base using BERT\n",
    "def map_fields_to_knowledge_base(form_fields, knowledge_base):\n",
    "    field_mappings = {}\n",
    "    knowledge_base_keys = list(knowledge_base.keys())\n",
    "    \n",
    "    # Encode all knowledge base keys\n",
    "    kb_embeddings = torch.cat([encode_text(key) for key in knowledge_base_keys])\n",
    "\n",
    "    for form_field in form_fields:\n",
    "        field_name = form_field['field_name']\n",
    "        form_embedding = encode_text(field_name)\n",
    "        \n",
    "        # Reshape embeddings to 2D arrays\n",
    "        form_embedding_2d = form_embedding.numpy().reshape(1, -1)\n",
    "        kb_embeddings_2d = kb_embeddings.numpy()\n",
    "        \n",
    "        # Calculate cosine similarity between form field and knowledge base keys\n",
    "        similarities = cosine_similarity(form_embedding_2d, kb_embeddings_2d)\n",
    "        \n",
    "        # Find the best match based on the highest similarity score\n",
    "        best_match_idx = similarities.argmax()\n",
    "        best_match_key = knowledge_base_keys[best_match_idx]\n",
    "        best_match_value = knowledge_base[best_match_key]\n",
    "        \n",
    "        # Store the best match\n",
    "        field_mappings[field_name] = best_match_value\n",
    "\n",
    "    return field_mappings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Process the PDF for filling form fields\n",
    "def pdf_to_images(pdf_path, output_dir):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    images = []\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc[page_num]\n",
    "        pix = page.get_pixmap(matrix=fitz.Matrix(3, 3))  # Increase resolution\n",
    "        img_path = os.path.join(output_dir, f'temp_page_{page_num + 1}.png')\n",
    "        pix.save(img_path)\n",
    "        images.append(img_path)\n",
    "    doc.close()\n",
    "    return images\n",
    "\n",
    "def preprocess_image(image):\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Apply adaptive thresholding\n",
    "    thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "    # Denoise\n",
    "    denoised = cv2.fastNlMeansDenoising(thresh, None, 10, 7, 21)\n",
    "    return denoised\n",
    "\n",
    "def detect_cells(gray):\n",
    "    _, thresh = cv2.threshold(gray, 200, 255, cv2.THRESH_BINARY_INV)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cells = []\n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt)\n",
    "        if 3000 < area < 200000:\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            if w > 50 and h > 20:\n",
    "                cells.append((x, y, w, h))\n",
    "    return cells\n",
    "\n",
    "def is_cell_empty(img, x, y, w, h):\n",
    "    cell = img[y:y+h, x:x+w]\n",
    "    gray = cv2.cvtColor(cell, cv2.COLOR_BGR2GRAY)\n",
    "    _, binary = cv2.threshold(gray, 250, 255, cv2.THRESH_BINARY)\n",
    "    white_pixel_ratio = np.sum(binary == 255) / (w * h)\n",
    "    return white_pixel_ratio > 0.95\n",
    "\n",
    "def get_field_name(img, x, y, w, h):\n",
    "    # Check the entire left side for the field name\n",
    "    left_cell = img[y:y+h, 0:x]\n",
    "    left_text = pytesseract.image_to_string(left_cell)\n",
    "    return left_text.strip() if left_text.strip() else \"Unknown Field\"\n",
    "\n",
    "def put_text_in_box(img, text, x, y, w, h, color=(0, 0, 0), align_left=False, align_top=False):\n",
    "    # Convert OpenCV image to PIL Image\n",
    "    pil_img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    draw = ImageDraw.Draw(pil_img)\n",
    "    \n",
    "    # Format the text to fit the field\n",
    "    formatted_text, font_size = format_text_as_in_pdf(text, w, h)\n",
    "    \n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", font_size)\n",
    "    except IOError:\n",
    "        font = ImageFont.load_default()\n",
    "    \n",
    "    margin = 5\n",
    "    line_spacing = 8  # Increased line spacing\n",
    "    \n",
    "    # Split text into lines if it's too wide\n",
    "    lines = []\n",
    "    words = formatted_text.split()\n",
    "    current_line = words[0]\n",
    "    for word in words[1:]:\n",
    "        bbox = draw.textbbox((0, 0), current_line + \" \" + word, font=font)\n",
    "        if bbox[2] - bbox[0] <= w - 2*margin:\n",
    "            current_line += \" \" + word\n",
    "        else:\n",
    "            lines.append(current_line)\n",
    "            current_line = word\n",
    "    lines.append(current_line)\n",
    "    \n",
    "    # Calculate total text height\n",
    "    bbox = draw.textbbox((0, 0), \"A\", font=font)\n",
    "    line_height = bbox[3] - bbox[1] + line_spacing\n",
    "    total_text_height = len(lines) * line_height - line_spacing\n",
    "    \n",
    "    # Draw text\n",
    "    for i, line in enumerate(lines):\n",
    "        if align_top:\n",
    "            text_y = y + margin + i * line_height\n",
    "        else:\n",
    "            text_y = y + (h - total_text_height) // 2 + i * line_height\n",
    "        \n",
    "        bbox = draw.textbbox((0, 0), line, font=font)\n",
    "        if align_left:\n",
    "            text_x = x + margin\n",
    "        else:\n",
    "            text_x = x + (w - (bbox[2] - bbox[0])) // 2\n",
    "        \n",
    "        # Draw text with a slight offset to create a bold effect\n",
    "        for offset in [(0, 0), (1, 0), (0, 1), (1, 1)]:\n",
    "            draw.text((text_x + offset[0], text_y + offset[1]), line, font=font, fill=color)\n",
    "    \n",
    "    # Convert back to OpenCV image\n",
    "    return cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "def format_text_as_in_pdf(text, field_width, field_height, max_font_size=38, min_font_size=10):\n",
    "    from PIL import ImageFont, ImageDraw, Image\n",
    "    \n",
    "    # Create a dummy image to calculate text size\n",
    "    dummy_img = Image.new('RGB', (field_width, field_height))\n",
    "    draw = ImageDraw.Draw(dummy_img)\n",
    "    \n",
    "    # Start with the maximum font size and decrease until the text fits\n",
    "    font_size = max_font_size\n",
    "    while font_size >= min_font_size:\n",
    "        try:\n",
    "            font = ImageFont.truetype(\"arial.ttf\", font_size)\n",
    "        except IOError:\n",
    "            font = ImageFont.load_default()\n",
    "        \n",
    "        # Calculate the bounding box of the text\n",
    "        bbox = draw.textbbox((0, 0), text, font=font)\n",
    "        text_width = bbox[2] - bbox[0]\n",
    "        text_height = bbox[3] - bbox[1]\n",
    "        \n",
    "        # Check if the text fits within the field\n",
    "        if text_width <= field_width and text_height <= field_height:\n",
    "            break\n",
    "        \n",
    "        # Decrease the font size\n",
    "        font_size -= 1\n",
    "    \n",
    "    return text, font_size\n",
    "\n",
    "def create_text_box_below(img, x, y, w, h, text, color=(0, 0, 0)):\n",
    "    # Define the new box coordinates below the existing one\n",
    "    new_y = y + h + 10  # 10 pixels below the current box\n",
    "    new_h = h  # Same height as the current box\n",
    "    new_x = x  # Same x-coordinate\n",
    "    new_w = w  # Same width as the current box\n",
    "\n",
    "    # Draw the new text box\n",
    "    img = put_text_in_box(img, text, new_x, new_y, new_w, new_h, color=color)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filled page saved: output_images\\filled_page_1.png\n",
      "Process completed!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # PDF file path\n",
    "    pdf_path = \"Dummy_Questionnaire.pdf\"\n",
    "    output_dir = \"output_images\"\n",
    "\n",
    "    # Knowledge base file path\n",
    "    knowledge_base_path = \"dummy_data.txt\"\n",
    "    \n",
    "    # Ensure the paths are correct\n",
    "    if not os.path.exists(pdf_path):\n",
    "        raise FileNotFoundError(f\"PDF file not found: {pdf_path}\")\n",
    "    if not os.path.exists(knowledge_base_path):\n",
    "        raise FileNotFoundError(f\"Knowledge base file not found: {knowledge_base_path}\")\n",
    "\n",
    "    # Extract form fields from the PDF\n",
    "    form_fields = extract_form_fields_from_pdf(pdf_path)\n",
    "\n",
    "    # Parse the knowledge base\n",
    "    knowledge_base = parse_knowledge_base(knowledge_base_path)\n",
    "\n",
    "    # Map form fields to knowledge base entries using BERT\n",
    "    field_mappings = map_fields_to_knowledge_base(form_fields, knowledge_base)\n",
    "    \n",
    "    # Convert PDF to images\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    image_paths = pdf_to_images(pdf_path, output_dir)\n",
    "    \n",
    "    for page_num, image_path in enumerate(image_paths):\n",
    "        img = cv2.imread(image_path)\n",
    "        preprocessed = preprocess_image(img)\n",
    "        cells = detect_cells(preprocessed)\n",
    "        \n",
    "        # Fill the form fields in the image sequentially\n",
    "        for field_name, fill_text in field_mappings.items():\n",
    "            for cell in cells:\n",
    "                x, y, w, h = cell\n",
    "                if is_cell_empty(img, x, y, w, h):\n",
    "                    current_field_name = get_field_name(img, x, y, w, h)\n",
    "                    if current_field_name == field_name:\n",
    "                        # Ensure the text is formatted as shown in Dummy_Questionnaire_Desired_Output.pdf\n",
    "                        formatted_text, font_size = format_text_as_in_pdf(fill_text, w, h)  # Pass width and height\n",
    "                        img = put_text_in_box(img, formatted_text, x, y, w, h)\n",
    "                        break           \n",
    "        \n",
    "        # Save the filled image\n",
    "        output_image_path = os.path.join(output_dir, f\"filled_page_{page_num + 1}.png\")\n",
    "        cv2.imwrite(output_image_path, img)\n",
    "        print(f\"Filled page saved: {output_image_path}\")\n",
    "\n",
    "    print(\"Process completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the Tesseract executable\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'  # Adjust this path as needed\n",
    "\n",
    "def pdf_to_images(pdf_path, output_dir):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    images = []\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc[page_num]\n",
    "        pix = page.get_pixmap(matrix=fitz.Matrix(3, 3))  # Increase resolution\n",
    "        img_path = os.path.join(output_dir, f'temp_page_{page_num + 1}.png')\n",
    "        pix.save(img_path)\n",
    "        images.append(img_path)\n",
    "    doc.close()\n",
    "    return images\n",
    "\n",
    "def preprocess_image(image):\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply adaptive thresholding\n",
    "    thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "    \n",
    "    # Denoise\n",
    "    denoised = cv2.fastNlMeansDenoising(thresh, None, 10, 7, 21)\n",
    "    \n",
    "    return denoised\n",
    "\n",
    "def detect_cells(gray):\n",
    "    _, thresh = cv2.threshold(gray, 200, 255, cv2.THRESH_BINARY_INV)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cells = []\n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt)\n",
    "        if 3000 < area < 200000:\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            if w > 50 and h > 20:\n",
    "                cells.append((x, y, w, h))\n",
    "    return cells\n",
    "\n",
    "def is_cell_empty(img, x, y, w, h):\n",
    "    cell = img[y:y+h, x:x+w]\n",
    "    gray = cv2.cvtColor(cell, cv2.COLOR_BGR2GRAY)\n",
    "    _, binary = cv2.threshold(gray, 250, 255, cv2.THRESH_BINARY)\n",
    "    white_pixel_ratio = np.sum(binary == 255) / (w * h)\n",
    "    return white_pixel_ratio > 0.95\n",
    "\n",
    "def get_field_name(img, x, y, w, h):\n",
    "    # Check the entire left side for the field name\n",
    "    left_cell = img[y:y+h, 0:x]\n",
    "    left_text = pytesseract.image_to_string(left_cell)\n",
    "    return left_text.strip() if left_text.strip() else \"Unknown Field\"\n",
    "\n",
    "def put_text_in_box(img, text, x, y, w, h, color=(0, 0, 0), font_size=38, thickness=2, align_left=False, align_top=False):\n",
    "    # Convert OpenCV image to PIL Image\n",
    "    pil_img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    draw = ImageDraw.Draw(pil_img)\n",
    "    \n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", font_size)\n",
    "    except IOError:\n",
    "        font = ImageFont.load_default()\n",
    "    \n",
    "    margin = 5\n",
    "    line_spacing = 8  # Increased line spacing\n",
    "    \n",
    "    # Split text into lines if it's too wide\n",
    "    lines = []\n",
    "    words = text.split()\n",
    "    current_line = words[0]\n",
    "    for word in words[1:]:\n",
    "        bbox = draw.textbbox((0, 0), current_line + \" \" + word, font=font)\n",
    "        if bbox[2] - bbox[0] <= w - 2*margin:\n",
    "            current_line += \" \" + word\n",
    "        else:\n",
    "            lines.append(current_line)\n",
    "            current_line = word\n",
    "    lines.append(current_line)\n",
    "    \n",
    "    # Calculate total text height\n",
    "    bbox = draw.textbbox((0, 0), \"A\", font=font)\n",
    "    line_height = bbox[3] - bbox[1] + line_spacing\n",
    "    total_text_height = len(lines) * line_height - line_spacing\n",
    "    \n",
    "    # Draw text\n",
    "    for i, line in enumerate(lines):\n",
    "        if align_top:\n",
    "            text_y = y + margin + i * line_height\n",
    "        else:\n",
    "            text_y = y + (h - total_text_height) // 2 + i * line_height\n",
    "        \n",
    "        bbox = draw.textbbox((0, 0), line, font=font)\n",
    "        if align_left:\n",
    "            text_x = x + margin\n",
    "        else:\n",
    "            text_x = x + (w - (bbox[2] - bbox[0])) // 2\n",
    "        \n",
    "        # Draw text with a slight offset to create a bold effect\n",
    "        for offset in [(0, 0), (1, 0), (0, 1), (1, 1)]:\n",
    "            draw.text((text_x + offset[0], text_y + offset[1]), line, font=font, fill=color)\n",
    "    \n",
    "    # Convert back to OpenCV image\n",
    "    return cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "def generate_dummy_answer(question):\n",
    "    # Generate a dummy answer based on the question\n",
    "    return f\"This is a dummy answer to the question: {question[:50]}...\"\n",
    "\n",
    "def detect_and_mark_cells(image_path, output_image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    # Preprocess the image\n",
    "    preprocessed = preprocess_image(img)\n",
    "    \n",
    "    cells = detect_cells(preprocessed)\n",
    "    \n",
    "    # Process cells\n",
    "    i = 0\n",
    "    while i < len(cells):\n",
    "        x, y, w, h = cells[i]\n",
    "        if is_cell_empty(img, x, y, w, h):\n",
    "            # Check for multiple empty cells\n",
    "            multi_cells = [cells[i]]\n",
    "            j = i + 1\n",
    "            while j < len(cells) and cells[j][1] == y and is_cell_empty(img, *cells[j]):\n",
    "                multi_cells.append(cells[j])\n",
    "                j += 1\n",
    "            \n",
    "            if len(multi_cells) > 1:\n",
    "                # Handle multi-value inputs\n",
    "                field_name = get_field_name(img, multi_cells[0][0], multi_cells[0][1], multi_cells[0][2], multi_cells[0][3])\n",
    "                \n",
    "                # Reverse the order of multi_cells to process them from left to right\n",
    "                multi_cells.reverse()\n",
    "                \n",
    "                for idx, cell in enumerate(multi_cells, 1):\n",
    "                    x, y, w, h = cell\n",
    "                    cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)  # Green for empty cells\n",
    "                    fill_text = f\"{{{field_name}_{idx}}}\"  # Add number to field name\n",
    "                    img = put_text_in_box(img, fill_text, x, y, w, h, font_size=24, align_left=True)  # Adjusted font size for multi-cells\n",
    "                \n",
    "                i = j - 1\n",
    "            else:\n",
    "                cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)  # Green for empty cells\n",
    "                field_name = get_field_name(img, x, y, w, h)\n",
    "                fill_text = f\"{{{field_name}}}\"\n",
    "                img = put_text_in_box(img, fill_text, x, y, w, h, font_size=28)  # Adjusted font size\n",
    "        else:\n",
    "            cv2.rectangle(img, (x, y), (x+w, y+h), (0, 0, 255), 2)  # Red for non-empty cells\n",
    "        i += 1\n",
    "\n",
    "    # Process entire image for Q. and A.\n",
    "    pil_img = Image.fromarray(cv2.cvtColor(preprocessed, cv2.COLOR_BGR2RGB))\n",
    "    data = pytesseract.image_to_data(pil_img, output_type=pytesseract.Output.DICT)\n",
    "    \n",
    "    i = 0\n",
    "    last_question = \"\"\n",
    "    while i < len(data['text']):\n",
    "        if data['text'][i].strip().lower().startswith('q.'):\n",
    "            start_x, start_y = data['left'][i], data['top'][i]\n",
    "            end_x, end_y = start_x + data['width'][i], start_y + data['height'][i]\n",
    "            \n",
    "            # Find the end of the question (next Q. or A.)\n",
    "            j = i + 1\n",
    "            while j < len(data['text']) and not (data['text'][j].strip().lower().startswith('q.') or data['text'][j].strip().lower().startswith('a.')):\n",
    "                end_x = max(end_x, data['left'][j] + data['width'][j])\n",
    "                end_y = max(end_y, data['top'][j] + data['height'][j])\n",
    "                j += 1\n",
    "            \n",
    "            # Adjust the question box size\n",
    "            start_x = max(0, start_x - 5)\n",
    "            start_y = max(0, start_y - 5)\n",
    "            end_x = min(img.shape[1], end_x + 5)\n",
    "            end_y = min(img.shape[0], end_y - int((end_y - start_y) * 0.2))  # Reduce height by 20%\n",
    "            \n",
    "            cv2.rectangle(img, (start_x, start_y), (end_x, end_y), (255, 0, 255), 2)  # Magenta for Q. questions\n",
    "            last_question = ' '.join(data['text'][i:j])\n",
    "            i = j - 1  # Move to the last processed word\n",
    "        \n",
    "        elif data['text'][i].strip().lower().startswith('a.'):\n",
    "            x, y = data['left'][i], data['top'][i]\n",
    "            w, h = data['width'][i], data['height'][i]\n",
    "            \n",
    "            # Create imaginary box that includes A. and extends below\n",
    "            img_h, img_w = img.shape[:2]\n",
    "            answer_box_w = int((img_w - (x + w + 3)) * 0.9)  # 10% shorter from the right side\n",
    "            \n",
    "            # Find the next element's y-coordinate\n",
    "            next_element_y = img_h\n",
    "            for j in range(i+1, len(data['text'])):\n",
    "                if data['text'][j].strip():\n",
    "                    next_element_y = data['top'][j]\n",
    "                    break\n",
    "            \n",
    "            answer_box_h = next_element_y - y - 10  # Leave a small gap\n",
    "            answer_box_y = y\n",
    "            cv2.rectangle(img, (x + w + 3, answer_box_y), (x + w + 3 + answer_box_w, answer_box_y + answer_box_h), (255, 255, 0), 2)  # Yellow for imaginary answer box\n",
    "            \n",
    "            # Fill answer box with dummy text\n",
    "            dummy_answer = generate_dummy_answer(last_question)\n",
    "            img = put_text_in_box(img, dummy_answer, x + w + 3, answer_box_y, answer_box_w, answer_box_h, align_left=True, align_top=True, font_size=28)  # Adjusted font size\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "    cv2.imwrite(output_image_path, img)\n",
    "    return img\n",
    "\n",
    "def process_pdf(input_pdf, output_dir):\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Convert PDF to images\n",
    "    print(\"Converting PDF to images...\")\n",
    "    image_paths = pdf_to_images(input_pdf, output_dir)\n",
    "\n",
    "    # Process each page\n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        print(f\"Processing page {i + 1}...\")\n",
    "        output_image_path = os.path.join(output_dir, f'marked_page_{i + 1}.png')\n",
    "        detect_and_mark_cells(image_path, output_image_path)\n",
    "        print(f\"Marked image for page {i + 1} saved to: {output_image_path}\")\n",
    "\n",
    "        # Clean up temporary image file\n",
    "        os.remove(image_path)\n",
    "\n",
    "    print(\"PDF processing complete.\")\n",
    "\n",
    "# Get the current script's directory\n",
    "script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "# Construct the paths\n",
    "input_pdf = os.path.join(script_dir, 'Dummy_Questionnaire.pdf')\n",
    "output_dir = os.path.join(script_dir, 'output')\n",
    "\n",
    "# Process the PDF\n",
    "process_pdf(input_pdf, output_dir)\n",
    "\n",
    "def parse_structured_data(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = file.read().split('\\n')\n",
    "    parsed_data = {}\n",
    "    for line in data:\n",
    "        if ':' in line:\n",
    "            key, value = line.split(':', 1)\n",
    "            parsed_data[key.strip()] = value.strip()\n",
    "    return parsed_data\n",
    "\n",
    "\n",
    "def extract_text_boxes(img):\n",
    "    h, w = img.shape[:2]\n",
    "    boxes = pytesseract.image_to_boxes(img)\n",
    "    text_boxes = []\n",
    "    for b in boxes.splitlines():\n",
    "        b = b.split(' ')\n",
    "        text_boxes.append({\n",
    "            'text': b[0],\n",
    "            'x': int(b[1]),\n",
    "            'y': h - int(b[2]),\n",
    "            'w': int(b[3]) - int(b[1]),\n",
    "            'h': int(b[4]) - int(b[2])\n",
    "        })\n",
    "    return text_boxes\n",
    "\n",
    "def find_empty_cells(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    empty_cells = []\n",
    "    for cnt in contours:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        if w > 20 and h > 20:  # Adjust these thresholds as needed\n",
    "            roi = thresh[y:y+h, x:x+w]\n",
    "            if cv2.countNonZero(roi) / (w * h) < 0.1:  # Adjust this threshold as needed\n",
    "                empty_cells.append((x, y, w, h))\n",
    "    return empty_cells\n",
    "\n",
    "def find_nearest_text(x, y, text_boxes):\n",
    "    nearest = None\n",
    "    min_distance = float('inf')\n",
    "    for box in text_boxes:\n",
    "        distance = ((box['x'] - x) ** 2 + (box['y'] - y) ** 2) ** 0.5\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            nearest = box['text']\n",
    "    return nearest\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and process the image\n",
    "    image_path = r\"C:\\Users\\varun\\Desktop\\output\\marked_page_1.png\"\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Error: Unable to read image from {image_path}\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Extract text boxes\n",
    "    text_boxes = extract_text_boxes(img)\n",
    "    \n",
    "    # Find empty cells\n",
    "    empty_cells = find_empty_cells(img)\n",
    "    \n",
    "    # Parse structured data\n",
    "    structured_data_path = \"Dummy_data.txt\"\n",
    "    structured_data = parse_structured_data(structured_data_path)\n",
    "\n",
    "    # Process fields and questions\n",
    "    fields_and_questions = {}\n",
    "    for box in text_boxes:\n",
    "        text = box['text'].strip()\n",
    "        if text.startswith(\"Field:\") or text.startswith(\"Q.\"):\n",
    "            fields_and_questions[text] = \"\"\n",
    "\n",
    "    # Use BERT to get answers and fill in empty cells\n",
    "    for key in fields_and_questions:\n",
    "        if key.startswith(\"Field:\"):\n",
    "            field_name = key[6:].strip()\n",
    "            if field_name in structured_data:\n",
    "                fields_and_questions[key] = structured_data[field_name]\n",
    "            else:\n",
    "                context = \" \".join(structured_data.values())\n",
    "                fields_and_questions[key] = get_bert_answer(f\"What is the {field_name}?\", context)\n",
    "        elif key.startswith(\"Q.\"):\n",
    "            question = key[2:].strip()\n",
    "            context = \" \".join(structured_data.values())\n",
    "            fields_and_questions[key] = get_bert_answer(question, context)\n",
    "\n",
    "    # Fill in empty cells with answers\n",
    "    for box in empty_cells:\n",
    "        x, y, w, h = box\n",
    "        nearest_text = find_nearest_text(x, y, text_boxes)\n",
    "        if nearest_text in fields_and_questions:\n",
    "            answer = fields_and_questions[nearest_text]\n",
    "            cv2.putText(img, answer, (x, y + h // 2), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
    "\n",
    "    # Save the overlayed image\n",
    "    cv2.imwrite(r\"C:\\Users\\varun\\Desktop\\output\\filled_form.jpg\", img)\n",
    "\n",
    "    # Print fields, questions, and answers\n",
    "    for key, value in fields_and_questions.items():\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting PDF to images...\n",
      "Processing page 1...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "put_text_in_box() got an unexpected keyword argument 'font_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 218\u001b[0m\n\u001b[0;32m    215\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(script_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;66;03m# Process the PDF\u001b[39;00m\n\u001b[1;32m--> 218\u001b[0m \u001b[43mprocess_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_pdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;66;03m# ... existing code ...\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 202\u001b[0m, in \u001b[0;36mprocess_pdf\u001b[1;34m(input_pdf, output_dir)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    201\u001b[0m output_image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmarked_page_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 202\u001b[0m \u001b[43mdetect_and_mark_cells\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_image_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfield_mappings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMarked image for page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_image_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    205\u001b[0m \u001b[38;5;66;03m# Clean up temporary image file\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 119\u001b[0m, in \u001b[0;36mdetect_and_mark_cells\u001b[1;34m(image_path, output_image_path, field_mappings)\u001b[0m\n\u001b[0;32m    117\u001b[0m         field_name \u001b[38;5;241m=\u001b[39m get_field_name(img, x, y, w, h)\n\u001b[0;32m    118\u001b[0m         fill_text \u001b[38;5;241m=\u001b[39m field_mappings\u001b[38;5;241m.\u001b[39mget(field_name, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfield_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 119\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mput_text_in_box\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfont_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Adjusted font size\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    121\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mrectangle(img, (x, y), (x\u001b[38;5;241m+\u001b[39mw, y\u001b[38;5;241m+\u001b[39mh), (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m), \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Red for non-empty cells\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: put_text_in_box() got an unexpected keyword argument 'font_size'"
     ]
    }
   ],
   "source": [
    "# ... existing imports ...\n",
    "\n",
    "# Set the path to the Tesseract executable\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'  # Adjust this path as needed\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')  # Corrected model name\n",
    "model = BertModel.from_pretrained('bert-base-uncased')  # Corrected model name\n",
    "\n",
    "# Step 1: Parse the Knowledge Base (dummy_data.txt)\n",
    "def parse_knowledge_base(file_path):\n",
    "    knowledge_base = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            match = re.match(r'(.+?):\\s*(.+)', line)\n",
    "            if match:\n",
    "                key, value = match.groups()\n",
    "                knowledge_base[key.strip()] = value.strip()\n",
    "    return knowledge_base\n",
    "\n",
    "# Step 2: Extract Form Fields from PDF\n",
    "def extract_form_fields_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    form_fields = []\n",
    "    for page_num in range(doc.page_count):\n",
    "        page = doc.load_page(page_num)\n",
    "        blocks = page.get_text(\"blocks\")\n",
    "        for block in blocks:\n",
    "            if len(block) >= 5:\n",
    "                x0, y0, x1, y1 = block[:4]\n",
    "                field_text = block[4].strip()\n",
    "                if field_text:\n",
    "                    form_fields.append({\n",
    "                        \"page\": page_num + 1,\n",
    "                        \"coordinates\": (x0, y0, x1, y1),\n",
    "                        \"field_name\": field_text\n",
    "                    })\n",
    "    return form_fields\n",
    "\n",
    "# Step 3: Use BERT to encode text\n",
    "def encode_text(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Mean pooling on token embeddings\n",
    "    embeddings = torch.mean(outputs.last_hidden_state, dim=1)\n",
    "    return embeddings\n",
    "\n",
    "# Step 4: Map Form Fields to Knowledge Base using BERT\n",
    "def map_fields_to_knowledge_base(form_fields, knowledge_base):\n",
    "    field_mappings = {}\n",
    "    knowledge_base_keys = list(knowledge_base.keys())\n",
    "    \n",
    "    # Encode all knowledge base keys\n",
    "    kb_embeddings = torch.cat([encode_text(key) for key in knowledge_base_keys])\n",
    "\n",
    "    for form_field in form_fields:\n",
    "        field_name = form_field['field_name']\n",
    "        form_embedding = encode_text(field_name)\n",
    "        \n",
    "        # Reshape embeddings to 2D arrays\n",
    "        form_embedding_2d = form_embedding.numpy().reshape(1, -1)\n",
    "        kb_embeddings_2d = kb_embeddings.numpy()\n",
    "        \n",
    "        # Calculate cosine similarity between form field and knowledge base keys\n",
    "        similarities = cosine_similarity(form_embedding_2d, kb_embeddings_2d)\n",
    "        \n",
    "        # Find the best match based on the highest similarity score\n",
    "        best_match_idx = similarities.argmax()\n",
    "        best_match_key = knowledge_base_keys[best_match_idx]\n",
    "        best_match_value = knowledge_base[best_match_key]\n",
    "        \n",
    "        # Store the best match\n",
    "        field_mappings[field_name] = best_match_value\n",
    "\n",
    "    return field_mappings\n",
    "\n",
    "# ... existing code ...\n",
    "\n",
    "def detect_and_mark_cells(image_path, output_image_path, field_mappings):\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    # Preprocess the image\n",
    "    preprocessed = preprocess_image(img)\n",
    "    \n",
    "    cells = detect_cells(preprocessed)\n",
    "    \n",
    "    # Process cells\n",
    "    i = 0\n",
    "    while i < len(cells):\n",
    "        x, y, w, h = cells[i]\n",
    "        if is_cell_empty(img, x, y, w, h):\n",
    "            # Check for multiple empty cells\n",
    "            multi_cells = [cells[i]]\n",
    "            j = i + 1\n",
    "            while j < len(cells) and cells[j][1] == y and is_cell_empty(img, *cells[j]):\n",
    "                multi_cells.append(cells[j])\n",
    "                j += 1\n",
    "            \n",
    "            if len(multi_cells) > 1:\n",
    "                # Handle multi-value inputs\n",
    "                field_name = get_field_name(img, multi_cells[0][0], multi_cells[0][1], multi_cells[0][2], multi_cells[0][3])\n",
    "                \n",
    "                # Reverse the order of multi_cells to process them from left to right\n",
    "                multi_cells.reverse()\n",
    "                \n",
    "                for idx, cell in enumerate(multi_cells, 1):\n",
    "                    x, y, w, h = cell\n",
    "                    cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)  # Green for empty cells\n",
    "                    fill_text = f\"{{{field_name}_{idx}}}\"  # Add number to field name\n",
    "                    img = put_text_in_box(img, fill_text, x, y, w, h, font_size=24, align_left=True)  # Adjusted font size for multi-cells\n",
    "                \n",
    "                i = j - 1\n",
    "            else:\n",
    "                cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)  # Green for empty cells\n",
    "                field_name = get_field_name(img, x, y, w, h)\n",
    "                fill_text = field_mappings.get(field_name, f\"{{{field_name}}}\")\n",
    "                img = put_text_in_box(img, fill_text, x, y, w, h, font_size=28)  # Adjusted font size\n",
    "        else:\n",
    "            cv2.rectangle(img, (x, y), (x+w, y+h), (0, 0, 255), 2)  # Red for non-empty cells\n",
    "        i += 1\n",
    "\n",
    "    # Process entire image for Q. and A.\n",
    "    pil_img = Image.fromarray(cv2.cvtColor(preprocessed, cv2.COLOR_BGR2RGB))\n",
    "    data = pytesseract.image_to_data(pil_img, output_type=pytesseract.Output.DICT)\n",
    "    \n",
    "    i = 0\n",
    "    last_question = \"\"\n",
    "    while i < len(data['text']):\n",
    "        if data['text'][i].strip().lower().startswith('q.'):\n",
    "            start_x, start_y = data['left'][i], data['top'][i]\n",
    "            end_x, end_y = start_x + data['width'][i], start_y + data['height'][i]\n",
    "            \n",
    "            # Find the end of the question (next Q. or A.)\n",
    "            j = i + 1\n",
    "            while j < len(data['text']) and not (data['text'][j].strip().lower().startswith('q.') or data['text'][j].strip().lower().startswith('a.')):\n",
    "                end_x = max(end_x, data['left'][j] + data['width'][j])\n",
    "                end_y = max(end_y, data['top'][j] + data['height'][j])\n",
    "                j += 1\n",
    "            \n",
    "            # Adjust the question box size\n",
    "            start_x = max(0, start_x - 5)\n",
    "            start_y = max(0, start_y - 5)\n",
    "            end_x = min(img.shape[1], end_x + 5)\n",
    "            end_y = min(img.shape[0], end_y - int((end_y - start_y) * 0.2))  # Reduce height by 20%\n",
    "            \n",
    "            cv2.rectangle(img, (start_x, start_y), (end_x, end_y), (255, 0, 255), 2)  # Magenta for Q. questions\n",
    "            last_question = ' '.join(data['text'][i:j])\n",
    "            i = j - 1  # Move to the last processed word\n",
    "        \n",
    "        elif data['text'][i].strip().lower().startswith('a.'):\n",
    "            x, y = data['left'][i], data['top'][i]\n",
    "            w, h = data['width'][i], data['height'][i]\n",
    "            \n",
    "            # Create imaginary box that includes A. and extends below\n",
    "            img_h, img_w = img.shape[:2]\n",
    "            answer_box_w = int((img_w - (x + w + 3)) * 0.9)  # 10% shorter from the right side\n",
    "            \n",
    "            # Find the next element's y-coordinate\n",
    "            next_element_y = img_h\n",
    "            for j in range(i+1, len(data['text'])):\n",
    "                if data['text'][j].strip():\n",
    "                    next_element_y = data['top'][j]\n",
    "                    break\n",
    "            \n",
    "            answer_box_h = next_element_y - y - 10  # Leave a small gap\n",
    "            answer_box_y = y\n",
    "            cv2.rectangle(img, (x + w + 3, answer_box_y), (x + w + 3 + answer_box_w, answer_box_y + answer_box_h), (255, 255, 0), 2)  # Yellow for imaginary answer box\n",
    "            \n",
    "            # Fill answer box with dummy text\n",
    "            dummy_answer = generate_dummy_answer(last_question)\n",
    "            img = put_text_in_box(img, dummy_answer, x + w + 3, answer_box_y, answer_box_w, answer_box_h, align_left=True, align_top=True, font_size=28)  # Adjusted font size\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "    cv2.imwrite(output_image_path, img)\n",
    "    return img\n",
    "\n",
    "def process_pdf(input_pdf, output_dir):\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Convert PDF to images\n",
    "    print(\"Converting PDF to images...\")\n",
    "    image_paths = pdf_to_images(input_pdf, output_dir)\n",
    "\n",
    "    # Extract form fields from the PDF\n",
    "    form_fields = extract_form_fields_from_pdf(input_pdf)\n",
    "\n",
    "    # Parse the knowledge base\n",
    "    knowledge_base_path = \"dummy_data.txt\"\n",
    "    knowledge_base = parse_knowledge_base(knowledge_base_path)\n",
    "\n",
    "    # Map form fields to knowledge base entries using BERT\n",
    "    field_mappings = map_fields_to_knowledge_base(form_fields, knowledge_base)\n",
    "\n",
    "    # Process each page\n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        print(f\"Processing page {i + 1}...\")\n",
    "        output_image_path = os.path.join(output_dir, f'marked_page_{i + 1}.png')\n",
    "        detect_and_mark_cells(image_path, output_image_path, field_mappings)\n",
    "        print(f\"Marked image for page {i + 1} saved to: {output_image_path}\")\n",
    "\n",
    "        # Clean up temporary image file\n",
    "        os.remove(image_path)\n",
    "\n",
    "    print(\"PDF processing complete.\")\n",
    "\n",
    "# Get the current working directory\n",
    "script_dir = os.getcwd()\n",
    "\n",
    "# Construct the paths\n",
    "input_pdf = os.path.join(script_dir, 'Dummy_Questionnaire.pdf')\n",
    "output_dir = os.path.join(script_dir, 'output')\n",
    "\n",
    "# Process the PDF\n",
    "process_pdf(input_pdf, output_dir)\n",
    "\n",
    "# ... existing code ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting PDF to images...\n",
      "Processing page 1...\n",
      "Marked image for page 1 saved to: c:\\Users\\varun\\Desktop\\RULE BASED PDF FORM FILLER\\output\\marked_page_1.png\n",
      "PDF processing complete.\n",
      "PDF saved to: output\\Marked_Document.pdf\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF for PDF processing\n",
    "import re  # For knowledge base parsing\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pytesseract\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Set the path to the Tesseract executable\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'  # Adjust this path as needed\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')  # Corrected model name\n",
    "model = BertModel.from_pretrained('bert-base-uncased')  # Corrected model name\n",
    "\n",
    "# Step 1: Parse the Knowledge Base (dummy_data.txt)\n",
    "def parse_knowledge_base(file_path):\n",
    "    knowledge_base = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            match = re.match(r'(.+?):\\s*(.+)', line)\n",
    "            if match:\n",
    "                key, value = match.groups()\n",
    "                knowledge_base[key.strip()] = value.strip()\n",
    "    return knowledge_base\n",
    "\n",
    "# Step 2: Extract Form Fields from PDF\n",
    "def extract_form_fields_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    form_fields = []\n",
    "    for page_num in range(doc.page_count):\n",
    "        page = doc.load_page(page_num)\n",
    "        blocks = page.get_text(\"blocks\")\n",
    "        for block in blocks:\n",
    "            if len(block) >= 5:\n",
    "                x0, y0, x1, y1 = block[:4]\n",
    "                field_text = block[4].strip()\n",
    "                if field_text:\n",
    "                    form_fields.append({\n",
    "                        \"page\": page_num + 1,\n",
    "                        \"coordinates\": (x0, y0, x1, y1),\n",
    "                        \"field_name\": field_text\n",
    "                    })\n",
    "    return form_fields\n",
    "\n",
    "# Step 3: Use BERT to encode text\n",
    "def encode_text(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Mean pooling on token embeddings\n",
    "    embeddings = torch.mean(outputs.last_hidden_state, dim=1)\n",
    "    return embeddings\n",
    "\n",
    "# Step 4: Map Form Fields to Knowledge Base using BERT\n",
    "def map_fields_to_knowledge_base(form_fields, knowledge_base):\n",
    "    field_mappings = {}\n",
    "    knowledge_base_keys = list(knowledge_base.keys())\n",
    "    \n",
    "    # Encode all knowledge base keys\n",
    "    kb_embeddings = torch.cat([encode_text(key) for key in knowledge_base_keys])\n",
    "\n",
    "    for form_field in form_fields:\n",
    "        field_name = form_field['field_name']\n",
    "        form_embedding = encode_text(field_name)\n",
    "        \n",
    "        # Reshape embeddings to 2D arrays\n",
    "        form_embedding_2d = form_embedding.numpy().reshape(1, -1)\n",
    "        kb_embeddings_2d = kb_embeddings.numpy()\n",
    "        \n",
    "        # Calculate cosine similarity between form field and knowledge base keys\n",
    "        similarities = cosine_similarity(form_embedding_2d, kb_embeddings_2d)\n",
    "        \n",
    "        # Find the best match based on the highest similarity score\n",
    "        best_match_idx = similarities.argmax()\n",
    "        best_match_key = knowledge_base_keys[best_match_idx]\n",
    "        best_match_value = knowledge_base[best_match_key]\n",
    "        \n",
    "        # Store the best match\n",
    "        field_mappings[field_name] = best_match_value\n",
    "\n",
    "    return field_mappings\n",
    "\n",
    "def pdf_to_images(pdf_path, output_dir):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    images = []\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc[page_num]\n",
    "        pix = page.get_pixmap(matrix=fitz.Matrix(3, 3))  # Increase resolution\n",
    "        img_path = os.path.join(output_dir, f'temp_page_{page_num + 1}.png')\n",
    "        pix.save(img_path)\n",
    "        images.append(img_path)\n",
    "    doc.close()\n",
    "    return images\n",
    "\n",
    "def preprocess_image(image):\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply adaptive thresholding\n",
    "    thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "    \n",
    "    # Denoise\n",
    "    denoised = cv2.fastNlMeansDenoising(thresh, None, 10, 7, 21)\n",
    "    \n",
    "    return denoised\n",
    "\n",
    "def detect_cells(gray):\n",
    "    _, thresh = cv2.threshold(gray, 200, 255, cv2.THRESH_BINARY_INV)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cells = []\n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt)\n",
    "        if 3000 < area < 200000:\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            if w > 50 and h > 20:\n",
    "                cells.append((x, y, w, h))\n",
    "    return cells\n",
    "\n",
    "def is_cell_empty(img, x, y, w, h):\n",
    "    cell = img[y:y+h, x:x+w]\n",
    "    gray = cv2.cvtColor(cell, cv2.COLOR_BGR2GRAY)\n",
    "    _, binary = cv2.threshold(gray, 250, 255, cv2.THRESH_BINARY)\n",
    "    white_pixel_ratio = np.sum(binary == 255) / (w * h)\n",
    "    return white_pixel_ratio > 0.95\n",
    "\n",
    "def get_field_name(img, x, y, w, h):\n",
    "    # Check the entire left side for the field name\n",
    "    left_cell = img[y:y+h, 0:x]\n",
    "    left_text = pytesseract.image_to_string(left_cell)\n",
    "    return left_text.strip() if left_text.strip() else \"Unknown Field\"\n",
    "\n",
    "def put_text_in_box(img, text, x, y, w, h, color=(0, 0, 0), font_size=38, thickness=2, align_left=False, align_top=False):\n",
    "    # Convert OpenCV image to PIL Image\n",
    "    pil_img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    draw = ImageDraw.Draw(pil_img)\n",
    "    \n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", font_size)\n",
    "    except IOError:\n",
    "        font = ImageFont.load_default()\n",
    "    \n",
    "    margin = 5\n",
    "    line_spacing = 8  # Increased line spacing\n",
    "    \n",
    "    # Split text into lines if it's too wide\n",
    "    lines = []\n",
    "    words = text.split()\n",
    "    current_line = words[0]\n",
    "    for word in words[1:]:\n",
    "        bbox = draw.textbbox((0, 0), current_line + \" \" + word, font=font)\n",
    "        if bbox[2] - bbox[0] <= w - 2*margin:\n",
    "            current_line += \" \" + word\n",
    "        else:\n",
    "            lines.append(current_line)\n",
    "            current_line = word\n",
    "    lines.append(current_line)\n",
    "    \n",
    "    # Calculate total text height\n",
    "    bbox = draw.textbbox((0, 0), \"A\", font=font)\n",
    "    line_height = bbox[3] - bbox[1] + line_spacing\n",
    "    total_text_height = len(lines) * line_height - line_spacing\n",
    "    \n",
    "    # Draw text\n",
    "    for i, line in enumerate(lines):\n",
    "        if align_top:\n",
    "            text_y = y + margin + i * line_height\n",
    "        else:\n",
    "            text_y = y + (h - total_text_height) // 2 + i * line_height\n",
    "        \n",
    "        bbox = draw.textbbox((0, 0), line, font=font)\n",
    "        if align_left:\n",
    "            text_x = x + margin\n",
    "        else:\n",
    "            text_x = x + (w - (bbox[2] - bbox[0])) // 2\n",
    "        \n",
    "        # Draw text with a slight offset to create a bold effect\n",
    "        for offset in [(0, 0), (1, 0), (0, 1), (1, 1)]:\n",
    "            draw.text((text_x + offset[0], text_y + offset[1]), line, font=font, fill=color)\n",
    "    \n",
    "    # Convert back to OpenCV image\n",
    "    return cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "def generate_dummy_answer(question):\n",
    "    # Generate a dummy answer based on the question\n",
    "    return f\"This is a dummy answer to the question: {question[:50]}...\"\n",
    "\n",
    "# ... existing imports ...\n",
    "\n",
    "def detect_and_mark_cells(image_path, output_image_path, field_mappings):\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    # Preprocess the image\n",
    "    preprocessed = preprocess_image(img)\n",
    "    \n",
    "    cells = detect_cells(preprocessed)\n",
    "    \n",
    "    # Process cells\n",
    "    i = 0\n",
    "    while i < len(cells):\n",
    "        x, y, w, h = cells[i]\n",
    "        if is_cell_empty(img, x, y, w, h):\n",
    "            # Check for multiple empty cells\n",
    "            multi_cells = [cells[i]]\n",
    "            j = i + 1\n",
    "            while j < len(cells) and cells[j][1] == y and is_cell_empty(img, *cells[j]):\n",
    "                multi_cells.append(cells[j])\n",
    "                j += 1\n",
    "            \n",
    "            if len(multi_cells) > 1:\n",
    "                # Handle multi-value inputs\n",
    "                field_name = get_field_name(img, multi_cells[0][0], multi_cells[0][1], multi_cells[0][2], multi_cells[0][3])\n",
    "                \n",
    "                # Reverse the order of multi_cells to process them from left to right\n",
    "                multi_cells.reverse()\n",
    "                \n",
    "                for idx, cell in enumerate(multi_cells, 1):\n",
    "                    x, y, w, h = cell\n",
    "                    cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)  # Green for empty cells\n",
    "                    fill_text = f\"{{{field_name}_{idx}}}\"  # Add number to field name\n",
    "                    img = put_text_in_box(img, fill_text, x, y, w, h, font_size=24, align_left=True)  # Adjusted font size for multi-cells\n",
    "                \n",
    "                i = j - 1\n",
    "            else:\n",
    "                cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)  # Green for empty cells\n",
    "                field_name = get_field_name(img, x, y, w, h)\n",
    "                fill_text = field_mappings.get(field_name, f\"{{{field_name}}}\")\n",
    "                img = put_text_in_box(img, fill_text, x, y, w, h, font_size=28)  # Adjusted font size\n",
    "        else:\n",
    "            cv2.rectangle(img, (x, y), (x+w, y+h), (0, 0, 255), 2)  # Red for non-empty cells\n",
    "        i += 1\n",
    "\n",
    "    # Process entire image for Q. and A.\n",
    "    pil_img = Image.fromarray(cv2.cvtColor(preprocessed, cv2.COLOR_BGR2RGB))\n",
    "    data = pytesseract.image_to_data(pil_img, output_type=pytesseract.Output.DICT)\n",
    "    \n",
    "    i = 0\n",
    "    last_question = \"\"\n",
    "    while i < len(data['text']):\n",
    "        if data['text'][i].strip().lower().startswith('q.'):\n",
    "            start_x, start_y = data['left'][i], data['top'][i]\n",
    "            end_x, end_y = start_x + data['width'][i], start_y + data['height'][i]\n",
    "            \n",
    "            # Find the end of the question (next Q. or A.)\n",
    "            j = i + 1\n",
    "            while j < len(data['text']) and not (data['text'][j].strip().lower().startswith('q.') or data['text'][j].strip().lower().startswith('a.')):\n",
    "                end_x = max(end_x, data['left'][j] + data['width'][j])\n",
    "                end_y = max(end_y, data['top'][j] + data['height'][j])\n",
    "                j += 1\n",
    "            \n",
    "            # Adjust the question box size\n",
    "            start_x = max(0, start_x - 5)\n",
    "            start_y = max(0, start_y - 5)\n",
    "            end_x = min(img.shape[1], end_x + 5)\n",
    "            end_y = min(img.shape[0], end_y - int((end_y - start_y) * 0.2))  # Reduce height by 20%\n",
    "            \n",
    "            cv2.rectangle(img, (start_x, start_y), (end_x, end_y), (255, 0, 255), 2)  # Magenta for Q. questions\n",
    "            last_question = ' '.join(data['text'][i:j])\n",
    "            i = j - 1  # Move to the last processed word\n",
    "        \n",
    "        elif data['text'][i].strip().lower().startswith('a.'):\n",
    "            x, y = data['left'][i], data['top'][i]\n",
    "            w, h = data['width'][i], data['height'][i]\n",
    "            \n",
    "            # Create imaginary box that includes A. and extends below\n",
    "            img_h, img_w = img.shape[:2]\n",
    "            answer_box_w = int((img_w - (x + w + 3)) * 0.9)  # 10% shorter from the right side\n",
    "            \n",
    "            # Find the next element's y-coordinate\n",
    "            next_element_y = img_h\n",
    "            for j in range(i+1, len(data['text'])):\n",
    "                if data['text'][j].strip():\n",
    "                    next_element_y = data['top'][j]\n",
    "                    break\n",
    "            \n",
    "            answer_box_h = next_element_y - y - 10  # Leave a small gap\n",
    "            answer_box_y = y\n",
    "            cv2.rectangle(img, (x + w + 3, answer_box_y), (x + w + 3 + answer_box_w, answer_box_y + answer_box_h), (255, 255, 0), 2)  # Yellow for imaginary answer box\n",
    "            \n",
    "            # Fill answer box with mapped value\n",
    "            field_name = last_question.split(':')[0].strip()  # Extract field name from the last question\n",
    "            mapped_value = field_mappings.get(field_name, \"No mapped value found\")\n",
    "            img = put_text_in_box(img, mapped_value, x + w + 3, answer_box_y, answer_box_w, answer_box_h, align_left=True, align_top=True, font_size=28)  # Adjusted font size\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "    cv2.imwrite(output_image_path, img)\n",
    "    return img\n",
    "\n",
    "# ... existing code ...\n",
    "def process_pdf(input_pdf, output_dir):\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Convert PDF to images\n",
    "    print(\"Converting PDF to images...\")\n",
    "    image_paths = pdf_to_images(input_pdf, output_dir)\n",
    "\n",
    "    # Extract form fields from the PDF\n",
    "    form_fields = extract_form_fields_from_pdf(input_pdf)\n",
    "\n",
    "    # Parse the knowledge base\n",
    "    knowledge_base_path = \"dummy_data.txt\"\n",
    "    knowledge_base = parse_knowledge_base(knowledge_base_path)\n",
    "\n",
    "    # Map form fields to knowledge base entries using BERT\n",
    "    field_mappings = map_fields_to_knowledge_base(form_fields, knowledge_base)\n",
    "\n",
    "    # Process each page\n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        print(f\"Processing page {i + 1}...\")\n",
    "        output_image_path = os.path.join(output_dir, f'marked_page_{i + 1}.png')\n",
    "        detect_and_mark_cells(image_path, output_image_path, field_mappings)\n",
    "        print(f\"Marked image for page {i + 1} saved to: {output_image_path}\")\n",
    "\n",
    "        # Clean up temporary image file\n",
    "        os.remove(image_path)\n",
    "\n",
    "    print(\"PDF processing complete.\")\n",
    "\n",
    "# Get the current script's directory\n",
    "script_dir = os.getcwd()\n",
    "\n",
    "# Construct the paths\n",
    "input_pdf = os.path.join(script_dir, 'Dummy_Questionnaire.pdf')\n",
    "output_dir = os.path.join(script_dir, 'output')\n",
    "\n",
    "# Process the PDF\n",
    "process_pdf(input_pdf, output_dir)\n",
    "\n",
    "def images_to_pdf(image_paths, output_pdf_path):\n",
    "    # Create a new PDF document\n",
    "    pdf_document = fitz.open()\n",
    "\n",
    "    for image_path in image_paths:\n",
    "        # Open the image using PIL\n",
    "        img = Image.open(image_path)\n",
    "        \n",
    "        # Convert the image to RGB if it's not\n",
    "        if img.mode != \"RGB\":\n",
    "            img = img.convert(\"RGB\")\n",
    "        \n",
    "        # Save the image to a temporary file in PDF format\n",
    "        temp_pdf_path = image_path.replace(\".png\", \".pdf\")\n",
    "        img.save(temp_pdf_path, \"PDF\", resolution=100.0)\n",
    "        \n",
    "        # Open the temporary PDF file and add it to the document\n",
    "        temp_pdf = fitz.open(temp_pdf_path)\n",
    "        pdf_document.insert_pdf(temp_pdf)\n",
    "        \n",
    "        # Close the temporary PDF file and remove it\n",
    "        temp_pdf.close()\n",
    "        os.remove(temp_pdf_path)\n",
    "    \n",
    "    # Save the final PDF document\n",
    "    pdf_document.save(output_pdf_path)\n",
    "    pdf_document.close()\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Directory containing the marked images\n",
    "    output_dir = \"output\"\n",
    "    \n",
    "    # List of image paths\n",
    "    image_paths = [os.path.join(output_dir, f) for f in sorted(os.listdir(output_dir)) if f.endswith(\".png\")]\n",
    "    \n",
    "    # Output PDF path\n",
    "    output_pdf_path = os.path.join(output_dir, \"Marked_Document.pdf\")\n",
    "    \n",
    "    # Convert images to PDF\n",
    "    images_to_pdf(image_paths, output_pdf_path)\n",
    "    print(f\"PDF saved to: {output_pdf_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 232\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPDF processing complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    231\u001b[0m \u001b[38;5;66;03m# Get the current script's directory\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m script_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;18;43m__file__\u001b[39;49m))\n\u001b[0;32m    234\u001b[0m script_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;18m__file__\u001b[39m))\n\u001b[0;32m    236\u001b[0m \u001b[38;5;66;03m# Construct the paths\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "# Set the path to the Tesseract executable\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'  # Adjust this path as needed\n",
    "\n",
    "def pdf_to_images(pdf_path, output_dir):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    images = []\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc[page_num]\n",
    "        pix = page.get_pixmap(matrix=fitz.Matrix(3, 3))  # Increase resolution\n",
    "        img_path = os.path.join(output_dir, f'temp_page_{page_num + 1}.png')\n",
    "        pix.save(img_path)\n",
    "        images.append(img_path)\n",
    "    doc.close()\n",
    "    return images\n",
    "\n",
    "def preprocess_image(image):\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply adaptive thresholding\n",
    "    thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "    \n",
    "    # Denoise\n",
    "    denoised = cv2.fastNlMeansDenoising(thresh, None, 10, 7, 21)\n",
    "    \n",
    "    return denoised\n",
    "\n",
    "def detect_cells(gray):\n",
    "    _, thresh = cv2.threshold(gray, 200, 255, cv2.THRESH_BINARY_INV)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cells = []\n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt)\n",
    "        if 3000 < area < 200000:\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            if w > 50 and h > 20:\n",
    "                cells.append((x, y, w, h))\n",
    "    return cells\n",
    "\n",
    "def is_cell_empty(img, x, y, w, h):\n",
    "    cell = img[y:y+h, x:x+w]\n",
    "    gray = cv2.cvtColor(cell, cv2.COLOR_BGR2GRAY)\n",
    "    _, binary = cv2.threshold(gray, 250, 255, cv2.THRESH_BINARY)\n",
    "    white_pixel_ratio = np.sum(binary == 255) / (w * h)\n",
    "    return white_pixel_ratio > 0.95\n",
    "\n",
    "def get_field_name(img, x, y, w, h):\n",
    "    # Check the entire left side for the field name\n",
    "    left_cell = img[y:y+h, 0:x]\n",
    "    left_text = pytesseract.image_to_string(left_cell)\n",
    "    return left_text.strip() if left_text.strip() else \"Unknown Field\"\n",
    "\n",
    "def put_text_in_box(img, text, x, y, w, h, color=(0, 0, 0), font_size=38, thickness=2, align_left=False, align_top=False):\n",
    "    # Convert OpenCV image to PIL Image\n",
    "    pil_img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    draw = ImageDraw.Draw(pil_img)\n",
    "    \n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", font_size)\n",
    "    except IOError:\n",
    "        font = ImageFont.load_default()\n",
    "    \n",
    "    margin = 5\n",
    "    line_spacing = 8  # Increased line spacing\n",
    "    \n",
    "    # Split text into lines if it's too wide\n",
    "    lines = []\n",
    "    words = text.split()\n",
    "    current_line = words[0]\n",
    "    for word in words[1:]:\n",
    "        bbox = draw.textbbox((0, 0), current_line + \" \" + word, font=font)\n",
    "        if bbox[2] - bbox[0] <= w - 2*margin:\n",
    "            current_line += \" \" + word\n",
    "        else:\n",
    "            lines.append(current_line)\n",
    "            current_line = word\n",
    "    lines.append(current_line)\n",
    "    \n",
    "    # Calculate total text height\n",
    "    bbox = draw.textbbox((0, 0), \"A\", font=font)\n",
    "    line_height = bbox[3] - bbox[1] + line_spacing\n",
    "    total_text_height = len(lines) * line_height - line_spacing\n",
    "    \n",
    "    # Draw text\n",
    "    for i, line in enumerate(lines):\n",
    "        if align_top:\n",
    "            text_y = y + margin + i * line_height\n",
    "        else:\n",
    "            text_y = y + (h - total_text_height) // 2 + i * line_height\n",
    "        \n",
    "        bbox = draw.textbbox((0, 0), line, font=font)\n",
    "        if align_left:\n",
    "            text_x = x + margin\n",
    "        else:\n",
    "            text_x = x + (w - (bbox[2] - bbox[0])) // 2\n",
    "        \n",
    "        # Draw text with a slight offset to create a bold effect\n",
    "        for offset in [(0, 0), (1, 0), (0, 1), (1, 1)]:\n",
    "            draw.text((text_x + offset[0], text_y + offset[1]), line, font=font, fill=color)\n",
    "    \n",
    "    # Convert back to OpenCV image\n",
    "    return cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "def generate_dummy_answer(question):\n",
    "    # Generate a dummy answer based on the question\n",
    "    return f\"This is a dummy answer to the question: {question[:50]}...\"\n",
    "\n",
    "# ... existing imports ...\n",
    "\n",
    "def detect_and_mark_cells(image_path, output_image_path, field_mappings):\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    # Preprocess the image\n",
    "    preprocessed = preprocess_image(img)\n",
    "    \n",
    "    cells = detect_cells(preprocessed)\n",
    "    \n",
    "    # Process cells\n",
    "    i = 0\n",
    "    while i < len(cells):\n",
    "        x, y, w, h = cells[i]\n",
    "        if is_cell_empty(img, x, y, w, h):\n",
    "            # Check for multiple empty cells\n",
    "            multi_cells = [cells[i]]\n",
    "            j = i + 1\n",
    "            while j < len(cells) and cells[j][1] == y and is_cell_empty(img, *cells[j]):\n",
    "                multi_cells.append(cells[j])\n",
    "                j += 1\n",
    "            \n",
    "            if len(multi_cells) > 1:\n",
    "                # Handle multi-value inputs\n",
    "                field_name = get_field_name(img, multi_cells[0][0], multi_cells[0][1], multi_cells[0][2], multi_cells[0][3])\n",
    "                \n",
    "                # Reverse the order of multi_cells to process them from left to right\n",
    "                multi_cells.reverse()\n",
    "                \n",
    "                for idx, cell in enumerate(multi_cells, 1):\n",
    "                    x, y, w, h = cell\n",
    "                    cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)  # Green for empty cells\n",
    "                    fill_text = f\"{{{field_name}_{idx}}}\"  # Add number to field name\n",
    "                    img = put_text_in_box(img, fill_text, x, y, w, h, font_size=24, align_left=True)  # Adjusted font size for multi-cells\n",
    "                \n",
    "                i = j - 1\n",
    "            else:\n",
    "                cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)  # Green for empty cells\n",
    "                field_name = get_field_name(img, x, y, w, h)\n",
    "                fill_text = field_mappings.get(field_name, f\"{{{field_name}}}\")\n",
    "                img = put_text_in_box(img, fill_text, x, y, w, h, font_size=28)  # Adjusted font size\n",
    "        else:\n",
    "            cv2.rectangle(img, (x, y), (x+w, y+h), (0, 0, 255), 2)  # Red for non-empty cells\n",
    "        i += 1\n",
    "\n",
    "    # Process entire image for Q. and A.\n",
    "    pil_img = Image.fromarray(cv2.cvtColor(preprocessed, cv2.COLOR_BGR2RGB))\n",
    "    data = pytesseract.image_to_data(pil_img, output_type=pytesseract.Output.DICT)\n",
    "    \n",
    "    i = 0\n",
    "    last_question = \"\"\n",
    "    while i < len(data['text']):\n",
    "        if data['text'][i].strip().lower().startswith('q.'):\n",
    "            start_x, start_y = data['left'][i], data['top'][i]\n",
    "            end_x, end_y = start_x + data['width'][i], start_y + data['height'][i]\n",
    "            \n",
    "            # Find the end of the question (next Q. or A.)\n",
    "            j = i + 1\n",
    "            while j < len(data['text']) and not (data['text'][j].strip().lower().startswith('q.') or data['text'][j].strip().lower().startswith('a.')):\n",
    "                end_x = max(end_x, data['left'][j] + data['width'][j])\n",
    "                end_y = max(end_y, data['top'][j] + data['height'][j])\n",
    "                j += 1\n",
    "            \n",
    "            # Adjust the question box size\n",
    "            start_x = max(0, start_x - 5)\n",
    "            start_y = max(0, start_y - 5)\n",
    "            end_x = min(img.shape[1], end_x + 5)\n",
    "            end_y = min(img.shape[0], end_y - int((end_y - start_y) * 0.2))  # Reduce height by 20%\n",
    "            \n",
    "            cv2.rectangle(img, (start_x, start_y), (end_x, end_y), (255, 0, 255), 2)  # Magenta for Q. questions\n",
    "            last_question = ' '.join(data['text'][i:j])\n",
    "            i = j - 1  # Move to the last processed word\n",
    "        \n",
    "        elif data['text'][i].strip().lower().startswith('a.'):\n",
    "            x, y = data['left'][i], data['top'][i]\n",
    "            w, h = data['width'][i], data['height'][i]\n",
    "            \n",
    "            # Create imaginary box that includes A. and extends below\n",
    "            img_h, img_w = img.shape[:2]\n",
    "            answer_box_w = int((img_w - (x + w + 3)) * 0.9)  # 10% shorter from the right side\n",
    "            \n",
    "            # Find the next element's y-coordinate\n",
    "            next_element_y = img_h\n",
    "            for j in range(i+1, len(data['text'])):\n",
    "                if data['text'][j].strip():\n",
    "                    next_element_y = data['top'][j]\n",
    "                    break\n",
    "            \n",
    "            answer_box_h = next_element_y - y - 10  # Leave a small gap\n",
    "            answer_box_y = y\n",
    "            cv2.rectangle(img, (x + w + 3, answer_box_y), (x + w + 3 + answer_box_w, answer_box_y + answer_box_h), (255, 255, 0), 2)  # Yellow for imaginary answer box\n",
    "            \n",
    "            # Fill answer box with mapped value\n",
    "            field_name = last_question.split(':')[0].strip()  # Extract field name from the last question\n",
    "            mapped_value = field_mappings.get(field_name, \"No mapped value found\")\n",
    "            img = put_text_in_box(img, mapped_value, x + w + 3, answer_box_y, answer_box_w, answer_box_h, align_left=True, align_top=True, font_size=28)  # Adjusted font size\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "    cv2.imwrite(output_image_path, img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def process_pdf(input_pdf, output_dir):\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Convert PDF to images\n",
    "    print(\"Converting PDF to images...\")\n",
    "    image_paths = pdf_to_images(input_pdf, output_dir)\n",
    "\n",
    "    # Process each page\n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        print(f\"Processing page {i + 1}...\")\n",
    "        output_image_path = os.path.join(output_dir, f'marked_page_{i + 1}.png')\n",
    "        detect_and_mark_cells(image_path, output_image_path)\n",
    "        print(f\"Marked image for page {i + 1} saved to: {output_image_path}\")\n",
    "\n",
    "        # Clean up temporary image file\n",
    "        os.remove(image_path)\n",
    "\n",
    "    print(\"PDF processing complete.\")\n",
    "\n",
    "# Get the current script's directory\n",
    "script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "# Construct the paths\n",
    "input_pdf = os.path.join(script_dir, 'Dummy_Questionnaire.pdf')\n",
    "output_dir = os.path.join(script_dir, 'output')\n",
    "\n",
    "# Process the PDF\n",
    "process_pdf(input_pdf, output_dir)\n",
    "\n",
    "def parse_structured_data(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = file.read().split('\\n')\n",
    "    parsed_data = {}\n",
    "    for line in data:\n",
    "        if ':' in line:\n",
    "            key, value = line.split(':', 1)\n",
    "            parsed_data[key.strip()] = value.strip()\n",
    "    return parsed_data\n",
    "\n",
    "\n",
    "def extract_text_boxes(img):\n",
    "    h, w = img.shape[:2]\n",
    "    boxes = pytesseract.image_to_boxes(img)\n",
    "    text_boxes = []\n",
    "    for b in boxes.splitlines():\n",
    "        b = b.split(' ')\n",
    "        text_boxes.append({\n",
    "            'text': b[0],\n",
    "            'x': int(b[1]),\n",
    "            'y': h - int(b[2]),\n",
    "            'w': int(b[3]) - int(b[1]),\n",
    "            'h': int(b[4]) - int(b[2])\n",
    "        })\n",
    "    return text_boxes\n",
    "\n",
    "def find_empty_cells(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    empty_cells = []\n",
    "    for cnt in contours:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        if w > 20 and h > 20:  # Adjust these thresholds as needed\n",
    "            roi = thresh[y:y+h, x:x+w]\n",
    "            if cv2.countNonZero(roi) / (w * h) < 0.1:  # Adjust this threshold as needed\n",
    "                empty_cells.append((x, y, w, h))\n",
    "    return empty_cells\n",
    "\n",
    "def find_nearest_text(x, y, text_boxes):\n",
    "    nearest = None\n",
    "    min_distance = float('inf')\n",
    "    for box in text_boxes:\n",
    "        distance = ((box['x'] - x) ** 2 + (box['y'] - y) ** 2) ** 0.5\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            nearest = box['text']\n",
    "    return nearest\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and process the image\n",
    "    image_path = r\"C:\\Users\\varun\\Desktop\\output\\marked_page_1.png\"\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Error: Unable to read image from {image_path}\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Extract text boxes\n",
    "    text_boxes = extract_text_boxes(img)\n",
    "    \n",
    "    # Find empty cells\n",
    "    empty_cells = find_empty_cells(img)\n",
    "    \n",
    "    # Parse structured data\n",
    "    structured_data_path = \"Dummy_data.txt\"\n",
    "    structured_data = parse_structured_data(structured_data_path)\n",
    "\n",
    "    # Process fields and questions\n",
    "    fields_and_questions = {}\n",
    "    for box in text_boxes:\n",
    "        text = box['text'].strip()\n",
    "        if text.startswith(\"Field:\") or text.startswith(\"Q.\"):\n",
    "            fields_and_questions[text] = \"\"\n",
    "\n",
    "    # Use BERT to get answers and fill in empty cells\n",
    "    for key in fields_and_questions:\n",
    "        if key.startswith(\"Field:\"):\n",
    "            field_name = key[6:].strip()\n",
    "            if field_name in structured_data:\n",
    "                fields_and_questions[key] = structured_data[field_name]\n",
    "            else:\n",
    "                context = \" \".join(structured_data.values())\n",
    "                fields_and_questions[key] = get_bert_answer(f\"What is the {field_name}?\", context)\n",
    "        elif key.startswith(\"Q.\"):\n",
    "            question = key[2:].strip()\n",
    "            context = \" \".join(structured_data.values())\n",
    "            fields_and_questions[key] = get_bert_answer(question, context)\n",
    "\n",
    "    # Fill in empty cells with answers\n",
    "    for box in empty_cells:\n",
    "        x, y, w, h = box\n",
    "        nearest_text = find_nearest_text(x, y, text_boxes)\n",
    "        if nearest_text in fields_and_questions:\n",
    "            answer = fields_and_questions[nearest_text]\n",
    "            cv2.putText(img, answer, (x, y + h // 2), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
    "\n",
    "    # Save the overlayed image\n",
    "    cv2.imwrite(r\"C:\\Users\\varun\\Desktop\\output\\filled_form.jpg\", img)\n",
    "\n",
    "    # Print fields, questions, and answers\n",
    "    for key, value in fields_and_questions.items():\n",
    "        print(f\"{key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
